# ğŸ§  Local RAG Application

A fully local **Retrieval-Augmented Generation (RAG)** system that answers questions strictly from the documents you provide â€” with **zero external internet dependency**.

This project combines semantic search, vector storage, and local LLM generation to create a private, offline AI knowledge assistant.

---

# ğŸš€ Features

* âœ… 100% Offline Operation
* âœ… Document chunking with metadata tracking
* âœ… Semantic search using embeddings
* âœ… Qdrant vector database for storage
* âœ… Ollama for local LLM text generation
* âœ… Strict answer grounding (refuses if answer is not in documents)
* âœ… CLI + Web UI (Streamlit)

---

# ğŸ“‹ Prerequisites

## 1ï¸âƒ£ Install Qdrant

### Docker Method (Recommended)

```bash
docker pull qdrant/qdrant
docker run -p 6333:6333 qdrant/qdrant
```

### OR Standalone Binary

Download from:
[https://github.com/qdrant/qdrant/releases](https://github.com/qdrant/qdrant/releases)

---

## 2ï¸âƒ£ Install Ollama

Download from:
[https://ollama.ai/download](https://ollama.ai/download)

After installation, pull a model:

```bash
ollama pull llama2
```

You can use other models as well (mistral, phi3, etc.).

---

## 3ï¸âƒ£ Install Python Dependencies

```bash
pip install -r requirements.txt
```

---

# ğŸ“ Project Structure

```
LOCAL_ADV_Rag/
â”œâ”€â”€ docs/                    # Store .txt documents
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ document_loader.py   # Load & chunk documents
â”‚   â”œâ”€â”€ embedder.py          # Generate embeddings
â”‚   â”œâ”€â”€ vector_store.py      # Qdrant integration
â”‚   â”œâ”€â”€ retriever.py         # Retrieve relevant chunks
â”‚   â””â”€â”€ rag_pipeline.py      # Main RAG pipeline
â”‚
â”œâ”€â”€ streamlit_app.py         # Streamlit Web UI
â”œâ”€â”€ STREAMLIT_GUIDE.md       # Streamlit documentation
â”œâ”€â”€ main.py                  # CLI entry point
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

# ğŸ§  Usage

## 1ï¸âƒ£ Add Documents

Place your `.txt` files inside:

```
docs/
```

---

## 2ï¸âƒ£ Index Documents

Run whenever documents are added or updated:

```bash
python main.py --index
```

---

## 3ï¸âƒ£ Ask Questions

```bash
python main.py --query "Your question here"
```

Example:

```bash
python main.py --query "What is Machine Learning?"
```

---

## 4ï¸âƒ£ Interactive Chat Mode

```bash
python main.py --interactive
```

---

# ğŸŒ Streamlit Web UI

This project also includes a **Streamlit-based web interface** for interactive querying.

---

## â–¶ï¸ Run the Streamlit App

```bash
streamlit run streamlit_app.py
```

Open in browser:

```
http://localhost:8501
```

---

## âœ¨ Streamlit Features

* Upload and manage documents
* Ask questions interactively
* View retrieved context chunks
* Grounded AI answers
* Source document tracking
* Clean web interface

---

## ğŸ“„ Streamlit File Reference

```
streamlit_app.py      â†’ Main Streamlit interface
STREAMLIT_GUIDE.md   â†’ Detailed setup & usage guide
```

For full instructions, see:

**STREAMLIT_GUIDE.md**

---

## ğŸ”„ Streamlit vs CLI

| Mode            | Command                          | Use Case       |
| --------------- | -------------------------------- | -------------- |
| CLI Query       | `python main.py --query`         | Quick queries  |
| Interactive CLI | `python main.py --interactive`   | Terminal chat  |
| Web UI          | `streamlit run streamlit_app.py` | User interface |

---

# âš™ï¸ How It Works

### 1. Document Loading

Reads all `.txt` files from `docs/`.

### 2. Chunking

Splits documents into **300â€“500 word chunks** with IDs.

### 3. Embedding

Converts chunks into vectors using Sentence-Transformers.

### 4. Storage

Stores embeddings in Qdrant with metadata.

### 5. Retrieval

Finds Top-K similar chunks via cosine similarity.

### 6. Generation

Ollama generates answers using retrieved context.

### 7. Validation

System refuses answers not grounded in documents.

---

# ğŸ”§ Configuration

Edit settings in **main.py**:

| Parameter       | Description      | Default          |
| --------------- | ---------------- | ---------------- |
| EMBEDDING_MODEL | Embedding model  | all-MiniLM-L6-v2 |
| LLM_MODEL       | Ollama model     | llama2           |
| CHUNK_SIZE      | Words per chunk  | 300â€“500          |
| TOP_K           | Retrieved chunks | 3                |

---

# ğŸ› ï¸ Troubleshooting

## Qdrant Connection Error

* Ensure Qdrant runs on port **6333**

Test:

```bash
curl http://localhost:6333/
```

---

## Ollama Connection Error

Check installation:

```bash
ollama list
```

---

## CUDA / GPU Errors

If GPU fails, run Ollama in CPU mode:

```bash
setx OLLAMA_NO_CUDA 1
```

Restart terminal afterward.

---

## No Documents Found

* Ensure files are in `docs/`
* Use `.txt` format only

---

# ğŸ“ Notes

* First run downloads embedding model (~80MB)
* Ollama models may be several GB
* All processing is local
* No internet required after setup

---

# ğŸ“¦ Releases & Packages

No releases published yet.

You can create versions via the **GitHub Releases** section when distributing updates.

---

# ğŸ‘¨â€ğŸ’» Author

Developed as a **Local Advanced RAG System** for private, offline document question answering.

---

â­ If you like this project, consider starring the repository!

